# üß† Knowledge Base Search Engine using Gemini LLM (RAG-based Document QA)

## **Objective**
This project enables intelligent search and question-answering across multiple PDF documents using **Retrieval-Augmented Generation (RAG)** powered by **Google‚Äôs Gemini LLM**.  
The goal is to provide concise, context-aware, and source-grounded answers to user queries drawn directly from the uploaded PDFs.

---

## **Scope of Work**
- **Input:** One or more PDF files uploaded by the user  
- **Process:** The documents are chunked, embedded, and stored in a **FAISS** vector index for efficient semantic retrieval  
- **Output:** The user submits a question, and the app returns a synthesized answer generated by the Gemini model, referencing the relevant document chunks  
- **Frontend:** A clean **Streamlit interface** allows document upload, query input, chat history, and theme toggle (dark/light mode)

---

## **Technical Expectations**
- **Backend & Retrieval:**
  - Uses **LangChain** + **FAISS** for efficient document retrieval
  - Embeddings generated via **HuggingFace model** (`intfloat/e5-base-v2`)
  - **Gemini API (Google Generative AI)** handles language understanding and synthesis
- **Frontend:**
  - Fully handled by **Streamlit** (no separate backend server required)
  - Includes persistent chat history, PDF-based retriever caching, and export options (**TXT**, **JSON**, **PDF**)
- **Environment:**
  - Store your API key in `.streamlit/secrets.toml` under:
    ```toml
    [secrets]
    GEMINI_API_KEY = "your_api_key_here"
    ```
  - Compatible with **Streamlit Cloud** and local deployment

---

## **LLM Usage Guidance**
**Example Prompt:**
> ‚ÄúUsing the uploaded documents, summarize and answer the following question succinctly.‚Äù

---

## **Deliverables**
- GitHub repository with all source files (`qa_app.py`, `retriever.py`, `llm.py`, `util.py`, etc.)
- Working **Streamlit** application
- 
---

## **Evaluation Focus**
- **Retrieval Accuracy:** How well the app finds relevant text chunks  
- **Answer Synthesis:** Relevance, clarity, and conciseness of LLM-generated answers  
- **Code Design:** Modularity, readability, and maintainability  
- **Integration Quality:** Smooth functioning between **LangChain**, **FAISS**, and **Gemini**

---

## **Example Run**
Run in Git Bash
# Clone the repository
git clone https://github.com/Deepak1125/Knowledge-base-Search-Engine.git

# Install dependencies
pip install -r requirements.txt

# Run the Streamlit app
streamlit run qa_app.py
